{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Topics with Spark LDA\n",
    "\n",
    "\n",
    "### Workflow of LDA_Pyspark.ipynb\n",
    "\n",
    "> <b> Load Cleaned Data (from Data_Cleaning.py) into Dataframe \n",
    "\n",
    "> <b> Vectorization: create tf-idf feature vectors\n",
    "\n",
    "In Spark Maching Learning Library, Tf-Idf is separated into two parts - TF and IDF to make them flexible. Therefore, CountVectorizer is used first to generate the term frequency vector. IDF then takes feature vectors created from CountVectorizer and scales each column (token), down-weighting columns (tokens) which appear frequently in a corpus.\n",
    "\n",
    "\n",
    "\n",
    "> <b> Train & Evaluate Base Model: train LDA model on complete dataset and evaluate\n",
    "    \n",
    "By training a base LDA model with complete dataset, a couple of topics would be discovered initially and evaluated by human judgement through visulization, which could be further improved by enriching stopwords and model parameter tunning.\n",
    "    \n",
    "> <b> Model Improvement: \n",
    "\n",
    "- Filtering with TF-IDF score: identifying reasonable threshold and adding underscored tokens to stoplist\n",
    "- Parameter Tuning:\n",
    "Dirichlet Parameters: Alpha, Beta, Number of Topics (k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.linalg import Vector, Vectors, DenseVector\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, CountVectorizerModel\n",
    "from pyspark.ml.clustering import LDA, LDAModel, LocalLDAModel\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Read from json file (single-line mode by records)\n",
    "cleaned = spark.read.json(\"cleaning_test_output.json\") \n",
    "cleaned.createOrReplaceTempView('cleaned')\n",
    "\n",
    "#Drop cleaned text of null values(852 rows removed, 134145 rows left)\n",
    "query = '''SELECT * FROM cleaned WHERE cleaned_text IS NOT NULL''' \n",
    "text = spark.sql(query)\n",
    "\n",
    "#Split cleaned text into column of tokens, create index column\n",
    "text = text.withColumn('tokens',F.split(F.trim(text.cleaned_text),\" \"))\\\n",
    "           .withColumn(\"index\", F.monotonically_increasing_id())\\\n",
    "           .select(\"*\")\n",
    "text.createOrReplaceTempView('cleaned_indexed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- body: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- modr_status: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- post_id: long (nullable = true)\n",
      " |-- post_key: string (nullable = true)\n",
      " |-- post_type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- index: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Schema of the text dataframe\n",
    "text.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(body='Is anyone pdl1 negative and taking opdivo or keytruda ?  Are you having good results?', cleaned_text='opdivo keytruda results ', confidence=1.0, lang='en', modr_status=2, name='Lung Cancer Survivors', post_id=749195, post_key='disc.749195', post_type='disc', title='Pdl1 negative', tokens=['opdivo', 'keytruda', 'results'], index=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preview of the first entry\n",
    "text.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF-IDF Feature Vectors\n",
    "\n",
    "#### Tf-Idf Sparse Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check vocabulary size for setting vocabSize parameters\n",
    "unfold_df = text.withColumn('word',F.explode(F.split(text.cleaned_text, \"\\s\"))).where('word != \"\"')\n",
    "vocab_df = unfold_df.groupBy(\"word\").agg(F.count(\"tokens\").alias('df')) #84084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134145 84084\n"
     ]
    }
   ],
   "source": [
    "#Create tfidf vectors (with all words)\n",
    "# TF\n",
    "query = '''SELECT tokens, index FROM cleaned_indexed''' \n",
    "df_text = spark.sql(query)\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=84084)\n",
    "cvModel = cv.fit(df_text)\n",
    "result_cv = cvModel.transform(df_text)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) \n",
    "\n",
    "#Check DocSize and VocabSize\n",
    "tfidf = result_tfidf.select('index','features')\n",
    "print(tfidf.count(), len(cvModel.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(index=0, features=SparseVector(84084, {41: 2.6916, 641: 5.0757, 1623: 5.9602}))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LDA Model on Complete Dataset and Evaluate\n",
    "\n",
    "#### Train LDA model with All Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Top Terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|topic|termIndices                                                 |termWeights                                                                                                                                                                                                                     |words                                                                                                  |weights                                                                         |\n",
      "+-----+------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|1    |[44, 2, 247, 34, 63, 287, 392, 82, 415, 52]                 |[0.022274818064038356, 0.014319748696248054, 0.01154849746670702, 0.010360131837283317, 0.009473937940514104, 0.007537523793294304, 0.006385184403439354, 0.005825612816545398, 0.005107793744805692, 0.005087784905214339]     |[thyroid, cancer, thyca, support, group, tsh, conference, research, page, information]                 |[0.0223, 0.0143, 0.0115, 0.0104, 0.0095, 0.0075, 0.0064, 0.0058, 0.0051, 0.0051]|\n",
      "|2    |[58, 255, 242, 305, 213, 2, 817, 232, 76, 816]              |[0.03997048384334887, 0.01979745548169033, 0.012978366811899742, 0.012777701126227274, 0.01123076929683364, 0.010901628234989285, 0.0075271450298286, 0.006963459646806463, 0.006867648403587322, 0.006275595767477625]         |[bladder, bcg, urine, urologist, kidney, cancer, cystoscopy, grade, treatments, catheter]              |[0.04, 0.0198, 0.013, 0.0128, 0.0112, 0.0109, 0.0075, 0.007, 0.0069, 0.0063]    |\n",
      "|3    |[87, 114, 107, 315, 248, 281, 412, 88, 61, 105]             |[0.012379822013179302, 0.009674150193512322, 0.008104840385694975, 0.007140830455999927, 0.006716209497988171, 0.0062943068988081064, 0.005818402240876263, 0.005455087975461037, 0.005041313083808234, 0.004720700442335713]   |[diet, food, water, foods, oil, drink, sugar, weight, liver, stomach]                                  |[0.0124, 0.0097, 0.0081, 0.0071, 0.0067, 0.0063, 0.0058, 0.0055, 0.005, 0.0047] |\n",
      "|4    |[856, 874, 1266, 1190, 1365, 1749, 1416, 1843, 1771, 1982]  |[0.015827890445586933, 0.012147327515255131, 0.008699194526306372, 0.006859618989481343, 0.006838888364510497, 0.0067195920115886605, 0.00669441526304613, 0.006606129936103218, 0.006418357157508588, 0.006056756347523043]    |[fmd, artery, arteries, cholesterol, tooth, carotid, aspirin, roller, stents, coaster]                 |[0.0158, 0.0121, 0.0087, 0.0069, 0.0068, 0.0067, 0.0067, 0.0066, 0.0064, 0.0061]|\n",
      "|5    |[4, 596, 27, 655, 280, 20, 67, 479, 1568, 237]              |[0.004625835624314994, 0.004407739933892844, 0.004360810014783963, 0.0036851405103774633, 0.003587663172462799, 0.003490679122055838, 0.003210405083782436, 0.0031152406272951486, 0.0029897654651070867, 0.0029836735591685713]|[day, sun, hope, voice, eyes, today, morning, book, beauty, light]                                     |[0.0046, 0.0044, 0.0044, 0.0037, 0.0036, 0.0035, 0.0032, 0.0031, 0.003, 0.003]  |\n",
      "|6    |[220, 188, 454, 409, 640, 620, 467, 621, 755, 707]          |[0.022515676115587906, 0.021823618956159425, 0.012742570733849073, 0.012157859048146783, 0.010841921587397716, 0.010761670828734762, 0.010416121662659575, 0.009420411662508293, 0.009354731908651031, 0.00902869743735759]     |[ivf, cycle, transfer, pregnancy, donor, embryos, eggs, egg, iui, fet]                                 |[0.0225, 0.0218, 0.0127, 0.0122, 0.0108, 0.0108, 0.0104, 0.0094, 0.0094, 0.009] |\n",
      "|7    |[551, 711, 178, 494, 985, 78, 141, 1269, 1396, 248]         |[0.01572221972693934, 0.011040294754087777, 0.01078719473545711, 0.009961382122135034, 0.009424181636132609, 0.009292940223110911, 0.008074863786256199, 0.007124868190481225, 0.006555596802921547, 0.0060947274917206706]     |[l, gleason, prostate, score, cbd, biopsy, psa, hep, cannabis, oil]                                    |[0.0157, 0.011, 0.0108, 0.01, 0.0094, 0.0093, 0.0081, 0.0071, 0.0066, 0.0061]   |\n",
      "|8    |[996, 2270, 954, 1003, 2243, 656, 2808, 3648, 1391, 215]    |[0.01682598399965443, 0.009492724893634046, 0.008910220710040594, 0.008230203313816131, 0.00679860855291815, 0.006338259697931751, 0.005585453794281434, 0.004648992857895089, 0.004508168766677473, 0.004174295802536571]      |[encephalitis, li, seizures, seizure, herpes, teeth, alzheimer, enfit, photos, tube]                   |[0.0168, 0.0095, 0.0089, 0.0082, 0.0068, 0.0063, 0.0056, 0.0046, 0.0045, 0.0042]|\n",
      "|9    |[298, 170, 139, 443, 456, 123, 125, 395, 524, 645]          |[0.01291641630393143, 0.012114210928122985, 0.011433066256166847, 0.0110982526460755, 0.010917684461022379, 0.010809448024786372, 0.010033787077939067, 0.00939108583762079, 0.00823672395473144, 0.007918790894045883]         |[education, parents, child, workshop, disabilities, school, children, training, participants, students]|[0.0129, 0.0121, 0.0114, 0.0111, 0.0109, 0.0108, 0.01, 0.0094, 0.0082, 0.0079]  |\n",
      "|10   |[12, 0, 17, 19, 36, 4, 79, 22, 48, 15]                      |[0.008193658771246014, 0.006655577490401521, 0.006004113212770223, 0.00584310904521678, 0.005815683985544406, 0.005813656284883492, 0.004982934479991815, 0.004465295903672571, 0.004448207386786814, 0.004263844688247651]     |[life, time, people, things, family, day, friends, way, love, work]                                    |[0.0082, 0.0067, 0.006, 0.0058, 0.0058, 0.0058, 0.005, 0.0045, 0.0044, 0.0043]  |\n",
      "|11   |[1, 21, 7, 3, 18, 57, 64, 5, 0, 35]                         |[0.018128922588977515, 0.007994593238772392, 0.005914047389649187, 0.005295058611875343, 0.004677769518530169, 0.004592402234051572, 0.004412428574420629, 0.0042658318481147, 0.004263001994936043, 0.004159448149391388]      |[pain, symptoms, doctor, years, blood, skin, mg, help, time, issues]                                   |[0.0181, 0.008, 0.0059, 0.0053, 0.0047, 0.0046, 0.0044, 0.0043, 0.0043, 0.0042] |\n",
      "|12   |[141, 178, 903, 759, 1439, 1796, 1698, 565, 1284, 2026]     |[0.0559809309885002, 0.023041077697940025, 0.013644267902376064, 0.012733142018821657, 0.008830058010532915, 0.008608412839436712, 0.008328441550173949, 0.007792020319650542, 0.005656297249404862, 0.005532946149227341]      |[psa, prostate, ibrance, lupron, testosterone, faslodex, pc, hormone, rise, prostatectomy]             |[0.056, 0.023, 0.0136, 0.0127, 0.0088, 0.0086, 0.0083, 0.0078, 0.0057, 0.0055]  |\n",
      "|13   |[588, 20, 878, 230, 496, 4, 48, 107, 1406, 1462]            |[0.014743105804687493, 0.012179865668374387, 0.01196425650006601, 0.011575593132057175, 0.010046367987746647, 0.00958119658853032, 0.008419452481977488, 0.007510728236842954, 0.006895381555395003, 0.006657156317046847]      |[hugs, today, debbie, pm, prayers, day, love, water, camp, tom]                                        |[0.0147, 0.0122, 0.012, 0.0116, 0.01, 0.0096, 0.0084, 0.0075, 0.0069, 0.0067]   |\n",
      "|14   |[54, 129, 403, 414, 142, 296, 350, 112, 24, 98]             |[0.009360037111437, 0.006959808504555134, 0.006227908788365537, 0.005745496841352037, 0.005704900703760293, 0.005194281314009931, 0.004894809367678959, 0.004837423810504967, 0.004603960802037094, 0.004470149634180874]       |[patients, study, osteoporosis, calcium, cells, vitamin, article, bone, disease, drug]                 |[0.0094, 0.007, 0.0062, 0.0057, 0.0057, 0.0052, 0.0049, 0.0048, 0.0046, 0.0045] |\n",
      "|15   |[2028, 1618, 3052, 1522, 4235, 3447, 3251, 4526, 5269, 4953]|[0.010469115963075676, 0.00720668958649918, 0.005596680252968386, 0.005502748761717042, 0.004298415532221472, 0.0036136882873051665, 0.003195677634609973, 0.0030372685975370273, 0.002996463751689023, 0.0029237588689893196]  |[tgab, orange, quest, agent, usc, varices, hiv, nite, myeloma, massages]                               |[0.0105, 0.0072, 0.0056, 0.0055, 0.0043, 0.0036, 0.0032, 0.003, 0.003, 0.0029]  |\n",
      "|16   |[46, 233, 33, 249, 342, 353, 45, 173, 207, 499]             |[0.012818897863838481, 0.008249024727488417, 0.007309822733106967, 0.006201459509010572, 0.0055985571233005595, 0.005587163299131666, 0.005341364800346619, 0.00484531689364696, 0.004698296981325956, 0.004521835224886067]    |[heart, oxygen, hospital, breath, pulmonary, breathing, lung, lungs, exercise, copd]                   |[0.0128, 0.0082, 0.0073, 0.0062, 0.0056, 0.0056, 0.0053, 0.0048, 0.0047, 0.0045]|\n",
      "|17   |[225, 269, 9, 549, 354, 97, 386, 512, 610, 627]             |[0.014604740715487714, 0.01357212133809773, 0.012496886709485806, 0.008080241759865764, 0.007947641931428365, 0.007820054194327386, 0.007685439812654936, 0.007563158236977372, 0.007058875738635947, 0.0069411568116181325]    |[bag, stoma, surgery, pouch, bowel, surgeon, colon, ostomy, colostomy, ileostomy]                      |[0.0146, 0.0136, 0.0125, 0.0081, 0.0079, 0.0078, 0.0077, 0.0076, 0.0071, 0.0069]|\n",
      "|18   |[475, 492, 545, 419, 608, 881, 845, 1139, 773, 1196]        |[0.012606970309614298, 0.01250362392172341, 0.0090620833732231, 0.00897552688263909, 0.007072960257927873, 0.006872065561092787, 0.0067533566042786005, 0.006041057121235416, 0.005800104421659294, 0.005405540080048947]       |[hpv, pap, patch, leak, sex, menopause, csf, leep, hysterectomy, cervix]                               |[0.0126, 0.0125, 0.0091, 0.009, 0.0071, 0.0069, 0.0068, 0.006, 0.0058, 0.0054]  |\n",
      "|19   |[556, 673, 484, 940, 68, 1535, 1432, 1761, 364, 639]        |[0.02034072175313431, 0.018442455593251236, 0.014465550674064858, 0.01211831728311786, 0.011152912434950325, 0.0071910919692042795, 0.006849401938848924, 0.006403278098617545, 0.006204550086780028, 0.0061848586596951]       |[tarceva, tagrisso, mutation, egfr, brain, afatinib, knife, chiari, progression, nsclc]                |[0.0203, 0.0184, 0.0145, 0.0121, 0.0112, 0.0072, 0.0068, 0.0064, 0.0062, 0.0062]|\n",
      "|20   |[2, 29, 66, 59, 10, 45, 85, 61, 60, 96]                     |[0.01566918359671818, 0.014620401045482545, 0.010774515119533902, 0.010751769244489184, 0.010048031912543678, 0.009139823416987295, 0.008705844995760865, 0.00867226699704425, 0.008345943710157781, 0.008304287467619673]      |[cancer, chemo, radiation, tumor, treatment, lung, scan, liver, stage, oncologist]                     |[0.0157, 0.0146, 0.0108, 0.0108, 0.01, 0.0091, 0.0087, 0.0087, 0.0083, 0.0083]  |\n",
      "+-----+------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "\n",
      "Topics: 20 Vocabulary: 84084\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "max_iterations = 100\n",
    "\n",
    "lda = LDA(featuresCol=\"features\",k=num_topics, seed=1, optimizer=\"online\", maxIter = max_iterations)\n",
    "ldaModel=lda.fit(tfidf)\n",
    "lda_df=ldaModel.transform(tfidf)\n",
    "\n",
    "ldatopics = ldaModel.describeTopics()\n",
    "numTopics = ldatopics.count()\n",
    "\n",
    "vocabulary = cvModel.vocabulary\n",
    "ListOfIndexToWords = F.udf(lambda wl: list([vocabulary[w] for w in wl]), ArrayType(StringType()))\n",
    "FormatNumbers = F.udf(lambda nl: [float(\"{:1.4f}\".format(x)) for x in nl], ArrayType(FloatType()))\n",
    "\n",
    "#Describe top 20 topics(10 top words per topic)\n",
    "toptopics = ldatopics.withColumn('topic',ldatopics.topic + 1)\\\n",
    "                     .withColumn('words', ListOfIndexToWords(ldatopics.termIndices))\\\n",
    "                     .withColumn('weights', FormatNumbers(ldatopics.termWeights))\n",
    "\n",
    "toptopics.show(truncate=False, n = numTopics)\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-190290427.02601972"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Log Likelihood of base LDA model \n",
    "ll = ldaModel.logLikelihood(tfidf)\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.434250933211537"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Log Perplexity of base LDA model\n",
    "lp = ldaModel.logPerplexity(tfidf)\n",
    "lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Base LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check if the model is distributed and save the model\n",
    "# print(ldaModel.isDistributed())\n",
    "# path = os.getcwd()\n",
    "# cvModel.save(path + 'CVModel_stem')\n",
    "# ldaModel.save(path + 'LDAModel_base')\n",
    "# lda.save(path + 'LDA_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvement (more meaningful topics)\n",
    "\n",
    "#### Generate TF-IDF-Term List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(index=0, term_list='results', feature_list=2.691643845918748)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = cvModel.vocabulary\n",
    "ListOfIndexToWords = F.udf(lambda wl: list([vocabulary[w] for w in wl]), ArrayType(StringType()))\n",
    "ExtractValues = F.udf(lambda vec: vec.values.tolist(), ArrayType(DoubleType()))\n",
    "ExtractIndex = F.udf(lambda vec: vec.indices.tolist(), ArrayType(IntegerType()))\n",
    "result_tfidf = result_tfidf.withColumn('feature_list',ExtractValues(result_tfidf.features))\\\n",
    "                           .withColumn('term_index', ExtractIndex(result_tfidf.features))\n",
    "result_tfidf = result_tfidf.withColumn('term_list', ListOfIndexToWords(result_tfidf.term_index))\n",
    "\n",
    "tfidf_termlist = result_tfidf.select('term_list','index','feature_list')\\\n",
    "                           .withColumn(\"tmp\", F.arrays_zip('term_list','feature_list'))\\\n",
    "                           .withColumn('tmp', F.explode('tmp'))\\\n",
    "                           .select('index',F.col('tmp.term_list'),F.col('tmp.feature_list'))\\\n",
    "                           .orderBy('index','feature_list')\n",
    "\n",
    "#Preview the first entry of TF-IDF-Term list\n",
    "tfidf_termlist.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['months', 'years', 'day', 'cancer', 'year', 'thanks', 'help',\n",
       "       'time', 'pain'], dtype=object)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get terms with tfidf <= 2\n",
    "tfidf_filter_2 = tfidf_termlist.filter(tfidf_termlist.feature_list <= 2).toPandas()['term_list'] #146126 tfidf <= 2#\n",
    "tfidf_filter_2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['results', 'symptoms', 'things', 'blood', 'disease', 'body',\n",
       "       'family', 'diagnosis', 'problems', 'treatment', 'husband', 'stage',\n",
       "       'lung', 'doctor', 'days', 'chemo', 'way', 'need', 'hope', 'post',\n",
       "       'doctors', 'care', 'love', 'share', 'test', 'hospital',\n",
       "       'information', 'time', 'health', 'weeks', 'advice', 'thing',\n",
       "       'heart', 'week', 'work', 'area', 'dr', 'question', 'surgery',\n",
       "       'people', 'lot', 'experience', 'times', 'morning', 'life', 'night',\n",
       "       'support', 'today', 'past', 'month', 'home', 'end', 'problem',\n",
       "       'effects', 'use', 'issues'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get terms with tfidf <= 3\n",
    "tfidf_filter_3 = tfidf_termlist.filter((tfidf_termlist.feature_list <= 3) & (tfidf_termlist.feature_list > 2)).toPandas()['term_list']\n",
    "tfidf_filter_3.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['help', 'march', 'feet', 'ideas', 'weight', 'appointment',\n",
       "       'medication', 'daughter', 'condition', 'pain', 'center',\n",
       "       'thoughts', 'suggestions', 'oncologist', 'water', 'check', 'guess',\n",
       "       'cell', 'effect', 'monday', 'friday', 'size', 'scan', 'advance',\n",
       "       'case', 'answer', 'cancer', 'pet', 'place', 'walk', 'kind', 'bed',\n",
       "       'mind', 'house', 'world', 'biopsy', 'feeling', 'point', 'friends',\n",
       "       'years', 'yesterday', 'situation', 'god', 'depression', 'head',\n",
       "       'radiation', 'neck', 'year', 'list', 'chest', 'doc', 'kids',\n",
       "       'meds', 'talk', 'tests', 'stomach', 'food', 'feel', 'thyroid',\n",
       "       'son', 'fact', 'breath', 'skin', 'hand', 'july', 'experiences',\n",
       "       'procedure', 'hours', 'bladder', 'dose', 'fatigue', 'syndrome',\n",
       "       'process', 'patients', 'follow', 'visit', 'june', 'breast',\n",
       "       'brain', 'diet', 'story', 'face', 'months', 'minutes', 'children',\n",
       "       'order', 'medications', 'questions', 'idea', 'pressure', 'lungs',\n",
       "       'room', 'august', 'term', 'treat', 'stay', 'half', 'legs',\n",
       "       'muscle', 'cause', 'light', 'right', 'mom', 'research', 'therapy',\n",
       "       'levels', 'study', 'wish', 'inspire', 'stuff', 'issue',\n",
       "       'treatments', 'type', 'person', 'deal', 'control', 'loss',\n",
       "       'reason', 'want', 'success', 'option', 'patient', 'january',\n",
       "       'tomorrow', 'hands', 'start', 'drugs', 'number', 'mg', 'period',\n",
       "       'wonder', 'clinic', 'ones', 'december', 'line', 'stories', 'day',\n",
       "       'community', 'friend', 'medicine', 'school', 'journey', 'changes',\n",
       "       'liver', 'options', 'nurse', 'baby', 'group', 'rest', 'october',\n",
       "       'age', 'specialist', 'infection', 'news', 'result', 'scans',\n",
       "       'lots', 'sleep', 'change', 'system', 'relief', 'leg', 'plan',\n",
       "       'job', 'site', 'thanks', 'state', 'risk', 'child', 'mother',\n",
       "       'opinion', 'testing', 'tissue', 'drug', 'report', 'tumor',\n",
       "       'anxiety', 'trial', 'bit', 'anybody', 'course', 'nausea',\n",
       "       'insurance', 'april', 'surgeon', 'bone', 'nodes', 'history',\n",
       "       'couple', 'level', 'wife', 'input', 'cycle', 'psoriasis', 'cells',\n",
       "       'info', 'office', 'chance', 'hour', 'worry', 'lymph', 'matter',\n",
       "       'stress', 'exercise', 'cm', 'grade'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get terms with tfidf <= 4\n",
    "tfidf_filter_4 = tfidf_termlist.filter((tfidf_termlist.feature_list <= 4) & (tfidf_termlist.feature_list > 3)).toPandas()['term_list']\n",
    "tfidf_filter_4.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Stopwords & Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords from Tfidf = 3\n",
    "noun_stpwd = \\\n",
    "            ['day','month','year','days','months','years','today','everyday',\n",
    "             'time', 'weeks','dr','times', 'thing','things','way', 'problems','thanks',\n",
    "            'husband','wife','doctor','doctors','care','results', 'symptoms', \n",
    "             'cancer', 'disease', 'body', 'family', 'diagnosis', 'treatment', \n",
    "             'husband', 'stage','help', 'need', 'hope', 'post','share', 'test',\n",
    "             'hospital','information', 'health', 'advice','heart', 'week', 'work', \n",
    "             'area', 'dr', 'question', 'surgery','people', 'pain', 'lot', 'experience',\n",
    "             'morning', 'life','night', 'support', 'past', 'home', 'end','problem', 'effects', 'use', 'issues',\n",
    "            'cm','mm','wanna', 'hour','info','april','anybody','lots','mom','state',\n",
    "            'list', 'doc', 'pm','am','october','plan','change','changes','result','options','ones',\n",
    "            'december','mg','period','start','number','january','patient','want','type','person',\n",
    "            'treatments','wish','stuff','right','issue','minutes','june','meds','son','july',\n",
    "            'hours','hour','god','friend','pet','march','daughter','thoughts','suggestions',\n",
    "            'guess','ideas','appointment','effect','world','feeling','point','monday','friday',\n",
    "            'bed','answer','kind','case','pharmacy','love','hugs','dad','house','place','child','li']\n",
    "\n",
    "#Words more than 1 letters (not common for medical abbreviations)\n",
    "one_characters = [word for word in cvModel.vocabulary if len(word) == 1]\n",
    "\n",
    "swRemover = StopWordsRemover(inputCol='tokens', outputCol=\"filtered\")\n",
    "swRemover.setStopWords(swRemover.getStopWords() + noun_stpwd + one_characters)\n",
    "\n",
    "df_text_new = swRemover.transform(df_text).select('index','filtered')\\\n",
    "                       .withColumn('tokens', F.col('filtered'))\\\n",
    "                       .select('index','tokens')\n",
    "\n",
    "#Save filtered dataframe\n",
    "# path = os.getcwd()\n",
    "# df_text_new.toPandas().to_json(path +'/filtered_text.json', lines = True, orient = 'records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-train Model and View Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=83920)\n",
    "cvModel_1 = cv.fit(df_text_new)\n",
    "result_cv_1 = cvModel_1.transform(df_text_new)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel_1 = idf.fit(result_cv_1)\n",
    "result_tfidf_1 = idfModel_1.transform(result_cv_1) \n",
    "\n",
    "#Check DocSize and VocabSize\n",
    "tfidf_new = result_tfidf_1.select('index','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save new tfidf\n",
    "# tfidf_new.write.json(\"tfidf_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|topic|termIndices                                                |termWeights                                                                                                                                                                                                                   |words                                                                                       |weights                                                                         |\n",
      "+-----+-----------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|1    |[8, 0, 43, 76, 196, 691, 1, 768, 50, 463]                  |[0.02584200339103232, 0.011577682340080473, 0.01139935115324836, 0.009848344451822295, 0.00780589794432341, 0.007665761290463677, 0.007561768326734404, 0.007384883575160645, 0.0065215149094735555, 0.006344761558139465]    |[liver, blood, lymph, nodes, recurrence, cirrhosis, chemo, ibrance, trial, onc]             |[0.0258, 0.0116, 0.0114, 0.0098, 0.0078, 0.0077, 0.0076, 0.0074, 0.0065, 0.0063]|\n",
      "|2    |[4, 185, 28, 50, 57, 49, 386, 470, 107, 51]                |[0.011818821233815198, 0.011730200694456787, 0.011433650988016068, 0.009518301340360961, 0.009315078571580652, 0.0085978536493306, 0.006494839122834014, 0.0063274486188727355, 0.006039327677357373, 0.005426183606029776]   |[patients, sarcoidosis, drug, trial, cells, study, methotrexate, fibrosis, drugs, cell]     |[0.0118, 0.0117, 0.0114, 0.0095, 0.0093, 0.0086, 0.0065, 0.0063, 0.006, 0.0054] |\n",
      "|3    |[980, 787, 1976, 1957, 110, 2212, 3262, 3389, 349, 3667]   |[0.01784384525061543, 0.013107338828414713, 0.00832058741264929, 0.008302349489149543, 0.005671928317266398, 0.005445751704809529, 0.004812805780242891, 0.0042649029367303505, 0.004237965277159402, 0.003750034167636229]   |[lymphedema, compression, cons, pros, leg, nivolumab, stockings, lipedema, wear, austin]    |[0.0178, 0.0131, 0.0083, 0.0083, 0.0057, 0.0054, 0.0048, 0.0043, 0.0042, 0.0038]|\n",
      "|4    |[132, 2, 9, 264, 148, 16, 287, 352, 441, 4]                |[0.012724948297601096, 0.012705289253104407, 0.011560271277446806, 0.007241899020928402, 0.006216731448080805, 0.005896741526010377, 0.005645109099875993, 0.005220832131111664, 0.004601507149756034, 0.004538490672832734]  |[thyca, thyroid, group, conference, meeting, research, page, awareness, survivors, patients]|[0.0127, 0.0127, 0.0116, 0.0072, 0.0062, 0.0059, 0.0056, 0.0052, 0.0046, 0.0045]|\n",
      "|5    |[721, 564, 883, 644, 1129, 739, 1814, 328, 1373, 2129]     |[0.00968639482966248, 0.00839619712401957, 0.007022554506055525, 0.006145162969547986, 0.005480813158026815, 0.0053204161010159045, 0.00469146075232694, 0.004486253038698866, 0.004336313045399022, 0.004297244653533422]    |[fmd, webinar, device, business, arteries, artery, utah, disabilities, webinars, li]        |[0.0097, 0.0084, 0.007, 0.0061, 0.0055, 0.0053, 0.0047, 0.0045, 0.0043, 0.0043] |\n",
      "|6    |[37, 177, 210, 275, 88, 159, 106, 20, 325, 256]            |[0.009276729090582235, 0.007831175364763965, 0.006995049391023163, 0.006939989736744944, 0.006064556440519062, 0.005624993646704474, 0.005621585731109425, 0.005207352122536659, 0.00516129866871787, 0.005081192237241694]   |[bone, hip, knee, osteoporosis, muscle, spine, exercise, weight, muscles, joints]           |[0.0093, 0.0078, 0.007, 0.0069, 0.0061, 0.0056, 0.0056, 0.0052, 0.0052, 0.0051] |\n",
      "|7    |[56, 85, 22, 223, 4, 748, 233, 10, 80, 578]                |[0.024888715594263634, 0.015011106449041434, 0.008818529739278975, 0.00754778552901038, 0.00751897020077799, 0.006201735518949902, 0.006075728964756597, 0.005781613935866593, 0.005391804238992344, 0.005194711868738435]    |[psa, prostate, therapy, article, patients, ehlers, eds, radiation, risk, gleason]          |[0.0249, 0.015, 0.0088, 0.0075, 0.0075, 0.0062, 0.0061, 0.0058, 0.0054, 0.0052] |\n",
      "|8    |[44, 79, 13, 46, 90, 86, 41, 25, 114, 315]                 |[0.00659664047729859, 0.00604839574725939, 0.005425803958242798, 0.00522126593067426, 0.004967609190910007, 0.004448819994631514, 0.004300355959291873, 0.004046736765234727, 0.0038106895432095745, 0.0036906868381663955]   |[school, parents, friends, children, depression, job, anxiety, talk, kids, workshop]        |[0.0066, 0.006, 0.0054, 0.0052, 0.005, 0.0044, 0.0043, 0.004, 0.0038, 0.0037]   |\n",
      "|9    |[93, 71, 253, 533, 642, 596, 661, 950, 616, 1423]          |[0.027168947256584444, 0.01767125427492778, 0.016963401148552028, 0.015566660492776616, 0.011615091636939271, 0.011395764253946297, 0.011009221983100292, 0.009526402932749629, 0.009203953336358737, 0.007080751166044898]   |[breast, baby, milk, pump, feed, supply, nursing, formula, bottle, mac]                     |[0.0272, 0.0177, 0.017, 0.0156, 0.0116, 0.0114, 0.011, 0.0095, 0.0092, 0.0071]  |\n",
      "|10   |[120, 146, 421, 384, 480, 495, 138, 667, 474, 113]         |[0.02287448181227592, 0.020878148574648047, 0.012831271542161191, 0.011882350543855724, 0.009753434866055411, 0.00961507872203158, 0.009190649833114826, 0.008248527809593646, 0.008105176226977305, 0.007662196785247745]    |[bag, stoma, pouch, ostomy, colostomy, ileostomy, bcg, bags, hernia, tube]                  |[0.0229, 0.0209, 0.0128, 0.0119, 0.0098, 0.0096, 0.0092, 0.0082, 0.0081, 0.0077]|\n",
      "|11   |[370, 547, 833, 1701, 1061, 2259, 1852, 907, 1115, 1481]   |[0.015549926333087374, 0.012498548143171216, 0.008766874875906045, 0.004721986315544963, 0.004050733879887643, 0.00373513977713617, 0.0035788196220958002, 0.0035272505959386574, 0.003502002482849074, 0.003360273264784128] |[copd, smoking, smoke, emphysema, tree, cigarette, inspiration, australia, pack, inhaler]   |[0.0155, 0.0125, 0.0088, 0.0047, 0.0041, 0.0037, 0.0036, 0.0035, 0.0035, 0.0034]|\n",
      "|12   |[572, 638, 594, 1041, 51, 1064, 1982, 1277, 794, 893]      |[0.01751327635752477, 0.015923667499434916, 0.015235001248526468, 0.0132247926872491, 0.013152995610556497, 0.010259340652276214, 0.005950961128846689, 0.005853181360007669, 0.0057526934169437, 0.00507532962861117]        |[photo, mast, user, albums, cell, mcas, mastocytosis, aspirin, ml, pattern]                 |[0.0175, 0.0159, 0.0152, 0.0132, 0.0132, 0.0103, 0.006, 0.0059, 0.0058, 0.0051] |\n",
      "|13   |[2, 162, 212, 30, 55, 270, 221, 834, 440, 749]             |[0.03101160668217729, 0.02144297223957649, 0.01628705602958659, 0.012948642332850646, 0.012083176348450419, 0.010822636188953521, 0.009630507247969756, 0.008236013659739999, 0.0079735148308403, 0.00789771842092819]        |[thyroid, tsh, endo, dose, levels, labs, range, mcg, endocrinologist, iron]                 |[0.031, 0.0214, 0.0163, 0.0129, 0.0121, 0.0108, 0.0096, 0.0082, 0.008, 0.0079]  |\n",
      "|14   |[1494, 2530, 973, 1938, 2965, 3199, 2400, 3995, 2103, 3907]|[0.011457954567326685, 0.005682339773319524, 0.0052809898774549245, 0.004787475948761293, 0.00419736794374486, 0.003972742527601565, 0.003875431644421889, 0.003766459675565018, 0.003763755383181118, 0.0036007357103231227] |[preemie, blockers, babies, moms, hose, deb, cp, nashville, germany, vanderbilt]            |[0.0115, 0.0057, 0.0053, 0.0048, 0.0042, 0.004, 0.0039, 0.0038, 0.0038, 0.0036] |\n",
      "|15   |[126, 347, 364, 281, 326, 436, 574, 488, 72, 60]           |[0.014106043004860406, 0.010459394711407742, 0.010322829640530553, 0.009924260247125342, 0.009890225810616486, 0.007780800029161473, 0.007462115555163218, 0.007033755352308453, 0.006553957923465309, 0.006429752547415019]  |[scleroderma, hpv, pap, pregnancy, transfer, ana, fet, embryos, feet, hands]                |[0.0141, 0.0105, 0.0103, 0.0099, 0.0099, 0.0078, 0.0075, 0.007, 0.0066, 0.0064] |\n",
      "|16   |[1, 3, 10, 11, 7, 17, 39, 26, 0, 82]                       |[0.013151342610209638, 0.011245965168910793, 0.009125326041212805, 0.007353343875767569, 0.0072529483396619404, 0.007066001531955676, 0.006987204415612667, 0.0067811216936205804, 0.00622467450547971, 0.005180507407637194] |[chemo, lung, radiation, brain, tumor, scan, chest, oncologist, blood, lungs]               |[0.0132, 0.0112, 0.0091, 0.0074, 0.0073, 0.0071, 0.007, 0.0068, 0.0062, 0.0052] |\n",
      "|17   |[116, 92, 506, 291, 339, 324, 416, 369, 621, 489]          |[0.018916942273222184, 0.013658254041313073, 0.011180903095016218, 0.010445379324745095, 0.010124669569326845, 0.009789570704771482, 0.009684890066613638, 0.009499396777847094, 0.009282687926666568, 0.00926276955550789]   |[ivf, cycle, donor, leak, eggs, neurologist, patch, headache, iui, egg]                     |[0.0189, 0.0137, 0.0112, 0.0104, 0.0101, 0.0098, 0.0097, 0.0095, 0.0093, 0.0093]|\n",
      "|18   |[133, 19, 33, 38, 189, 284, 468, 403, 306, 8]              |[0.010391117952897466, 0.010266784162306421, 0.00895251152410411, 0.008246409295573694, 0.006829903783705818, 0.0068069829584138175, 0.005525846194756551, 0.005523266852147771, 0.005441936242875769, 0.005255664398605014]  |[oil, diet, water, food, foods, sugar, tea, youtube, coffee, liver]                         |[0.0104, 0.0103, 0.009, 0.0082, 0.0068, 0.0068, 0.0055, 0.0055, 0.0054, 0.0053] |\n",
      "|19   |[5, 34, 104, 67, 156, 161, 165, 289, 60, 21]               |[0.012279777886093475, 0.012087605965039081, 0.007757271235038755, 0.006119171315265799, 0.005016646934513156, 0.0048808953025211745, 0.004849663462882262, 0.0047435779548209425, 0.004644637160530545, 0.004623361013366617]|[skin, psoriasis, hair, face, eyes, flare, eye, cream, hands, head]                         |[0.0123, 0.0121, 0.0078, 0.0061, 0.005, 0.0049, 0.0048, 0.0047, 0.0046, 0.0046] |\n",
      "|20   |[6, 12, 179, 130, 109, 138, 302, 123, 27, 125]             |[0.025023720301773904, 0.013103060599337005, 0.010331098993722488, 0.009661657114978187, 0.008107027976437968, 0.007997984542724275, 0.007257397317628065, 0.0072332778957720145, 0.007200930889813437, 0.006719318331379296] |[bladder, biopsy, urologist, urine, procedure, bcg, nodule, grade, surgeon, report]         |[0.025, 0.0131, 0.0103, 0.0097, 0.0081, 0.008, 0.0073, 0.0072, 0.0072, 0.0067]  |\n",
      "+-----+-----------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "\n",
      "Topics: 20 Vocabulary: 83920\n"
     ]
    }
   ],
   "source": [
    "#Re-train the model\n",
    "num_topics = 20\n",
    "max_iterations = 100\n",
    "\n",
    "lda_1 = LDA(featuresCol=\"features\",k=num_topics, seed=1, optimizer=\"online\", maxIter = max_iterations)\n",
    "ldaModel_1=lda_1.fit(tfidf_new)\n",
    "#lda_df_1=ldaModel_1.transform(tfidf_new)\n",
    "\n",
    "ldatopics_1 = ldaModel_1.describeTopics()\n",
    "numTopics = ldatopics_1.count()\n",
    "\n",
    "vocabulary_1 = cvModel_1.vocabulary\n",
    "ListOfIndexToWords = F.udf(lambda wl: list([vocabulary_1[w] for w in wl]), ArrayType(StringType()))\n",
    "FormatNumbers = F.udf(lambda nl: [float(\"{:1.4f}\".format(x)) for x in nl], ArrayType(FloatType()))\n",
    "\n",
    "#Describe top 20 topics(10 top words per topic)\n",
    "toptopics_1 = ldatopics_1.withColumn('topic',ldatopics_1.topic + 1)\\\n",
    "                     .withColumn('words', ListOfIndexToWords(ldatopics_1.termIndices))\\\n",
    "                     .withColumn('weights', FormatNumbers(ldatopics_1.termWeights))\n",
    "\n",
    "toptopics_1.show(truncate=False, n = numTopics)\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabulary_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(83920, 20, [7038.5126, 4597.086, 1520.6349, 184.8832, 724.8405, 72.6162, 30.8562, 2221.0819, ..., 0.1113, 5.7495, 0.4829, 0.0944, 0.2156, 0.1037, 0.1086, 0.1097], 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaModel_1.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check if the model is distributed and save the model\n",
    "print(ldaModel_1.isDistributed())\n",
    "path = os.getcwd()\n",
    "cvModel_1.save(path + 'CVModel_1')\n",
    "ldaModel_1.save(path + 'LDAModel_1')\n",
    "lda_1.save(path + 'LDA_base_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Log Likelihood of base LDA model \n",
    "ll = ldaModel.logLikelihood(tfidf)\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.32607268178269"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Log Perplexity of base LDA model \n",
    "lp = ldaModel_1.logPerplexity(tfidf_new)\n",
    "lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Exploration of the Generated Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model - ldaModel_1\n",
    "path = os.getcwd()\n",
    "usedLDAModel = LocalLDAModel.load(path + 'LDAModel_1')\n",
    "usedCVModel = CountVectorizerModel.load(path + 'CVModel_1')\n",
    "#Load saved filtered data\n",
    "df_text_new = spark.read.json(path+ '/filtered_text.json')\n",
    "#Load saved tfidf\n",
    "from pyspark.sql.types import StructField, StructType , LongType, StringType, IntegerType\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, SparseVector\n",
    "tfidfSchema = StructType([StructField(\"index\", LongType(), True), StructField(\"features\", VectorUDT(), True)])\n",
    "tfidf_new = spark.read.schema(tfidfSchema).json(\"tfidf_new\").sort(F.col(\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count docs for each topic\n",
    "countTopDocs = (usedLDAModel\n",
    "                .transform(tfidf_new)\n",
    "                .select(\"topicDistribution\")\n",
    "                .rdd.map(lambda r: Row( nTopTopic = int(np.argmax(r)))).toDF()\n",
    "                .groupBy(\"nTopTopic\").count().sort(\"nTopTopic\")) \\\n",
    "                .withColumn('topic', F.col('nTopTopic')+1) \\\n",
    "                .select('topic','count').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~xinyuan_0420/0 or inside your plot.ly account where it is named 'Topic Distribution across Topics'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~xinyuan_0420/0.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot \"Document Counts by Topic\"\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "data = [go.Bar(x=countTopDocs['topic'],\n",
    "            y=countTopDocs['count'].astype('str'))]\n",
    "\n",
    "py.iplot(data, filename='Topic Distribution across Topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83920"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyLDAVis\n",
    "#Required Input for prepare.py\n",
    "# topic_term_dists : array-like, shape (`n_topics`, `n_terms`)\n",
    "#         Matrix of topic-term probabilities. Where `n_terms` is `len(vocab)`.\n",
    "# doc_topic_dists : array-like, shape (`n_docs`, `n_topics`)\n",
    "#         Matrix of document-topic probabilities.\n",
    "# doc_lengths : array-like, shape `n_docs`\n",
    "#         The length of each document, i.e. the number of words in each document.\n",
    "#         The order of the numbers should be consistent with the ordering of the\n",
    "#         docs in `doc_topic_dists`.\n",
    "# vocab : array-like, shape `n_terms`\n",
    "#         List of all the words in the corpus used to train the model.\n",
    "# term_frequency : array-like, shape `n_terms`\n",
    "#         The count of each particular term over the entire corpus. The ordering\n",
    "#         of these counts should correspond with `vocab` and `topic_term_dists`.\n",
    "\n",
    "import pyLDAvis\n",
    "#topicsMatrix() - 'topic_term_dists'\n",
    "#topicDistribution - 'doc_topic_dists'\n",
    "#doc_length - tokens count\n",
    "#usedCVModel.vocabulary - 'vocab'\n",
    "\n",
    "usedLDAModel.vocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(index=0, tokens=['opdivo', 'keytruda'])]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doc length\n",
    "#cleaned.select('body').take(1)\n",
    "df_text_new.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LDA_model(ldamodel, cvmodel, doc_vect, doc_df):\n",
    "    def dense_to_array(v):\n",
    "        new_array = list([float(x) for x in v])\n",
    "        return new_array\n",
    "    dense_to_array_udf = F.udf(dense_to_array, ArrayType(FloatType()))\n",
    "    \n",
    "    def sparse_to_array(v):\n",
    "        v = DenseVector(v)\n",
    "        new_array = list([float(x) for x in v])\n",
    "        return new_array\n",
    "    sparse_to_array_udf = F.udf(sparse_to_array, ArrayType(FloatType()))\n",
    "    \n",
    "    topic_term_dists = ldamodel.topicsMatrix().toArray().tolist()\n",
    "    doc_topic_dists = ldaModel.transform(text_vect)\\\n",
    "                                           .withColumn('topicDistribution', dense_to_array_udf('topicDistribution'))\\\n",
    "                                           .select('topicDistribution').toPandas().topicDistribution.values.tolist()\n",
    "    doc_lengths = \n",
    "    vocab = cvmodel.vocabulary\n",
    "    term_frequency = cvmodel.transform(doc_df)\\\n",
    "                            .withColumn('raw_features', sparse_to_array_udf('raw_features'))\\\n",
    "                            .select('raw_features').toPandas().raw_features.values.tolist()\n",
    "    \n",
    "    data = {'topic_term_dists': topic_term_dists, \n",
    "            'doc_topic_dists': doc_topic_dists,\n",
    "            'doc_lengths': ,\n",
    "            'vocab': vocab,\n",
    "            'term_frequency': term_frequency }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_model_data = load_LDA_model('data/movie_reviews_input.json')\n",
    "\n",
    "print('Topic-Term shape: %s' % str(np.array(movies_model_data['topic_term_dists']).shape))\n",
    "print('Doc-Topic shape: %s' % str(np.array(movies_model_data['doc_topic_dists']).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify top docs for each topics\n",
    "countVectors = (result_cv.select(\"index\", \"raw_features\").cache())\n",
    "df = ldaModel.transform(countVectors)\n",
    "\n",
    "topWords = ldatopics.select(ListOfIndexToWords(ldatopics.termIndices).alias('words')).take(numTopics)\n",
    "\n",
    "#Show single top topic\n",
    "nTopDoc = 1  \n",
    "\n",
    "for i in range(0, numTopics):\n",
    "    ntopic = i  # which topic \n",
    "    print('Topic ' + str(ntopic) + '\\n')  \n",
    "\n",
    "    df_sliced = df.select(\"index\", \"topicDistribution\") \\\n",
    "        .rdd.map(lambda r: Row(ID=int(r[0]), weight=float(r[1][ntopic]))).toDF()\n",
    "\n",
    "    DocIDs = df_sliced.sort(df_sliced.weight.desc()).take(nTopDoc)\n",
    "    print('Top Document(s):',DocIDs)\n",
    "    for d_id in DocIDs:\n",
    "        df_text.filter(df_text.index == d_id[0]) \\\n",
    "            .select('title', 'body') \\\n",
    "            .show(truncate=False)\n",
    "\n",
    "    print('Top terms:')\n",
    "    print(topWords[ntopic][0], '\\n')\n",
    "    print('===================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
