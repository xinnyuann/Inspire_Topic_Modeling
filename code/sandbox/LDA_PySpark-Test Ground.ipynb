{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Read from json file (single-line mode by records)\n",
    "cleaned_text = spark.read.json(\"cleaning_test_output.json\") \n",
    "cleaned_text.createOrReplaceTempView('cleaned')\n",
    "\n",
    "#Drop cleaned text of null values(852 rows removed, 134145 rows left)\n",
    "query = '''SELECT * FROM cleaned WHERE cleaned_text IS NOT NULL''' \n",
    "text = spark.sql(query)\n",
    "\n",
    "#Split cleaned text into column of tokens, create index column\n",
    "text = text.withColumn('cleaned_text', F.trim(text.cleaned_text))\\\n",
    "           .withColumn('tokens',F.split(text.cleaned_text,\" \"))\\\n",
    "           .withColumn(\"index\", F.monotonically_increasing_id())\\\n",
    "           .select(\"*\")\n",
    "text.createOrReplaceTempView('cleaned_indexed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema of the text dataframe\n",
    "text.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview of the first entry\n",
    "text.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF-IDF Feature Vectors\n",
    "\n",
    "#### Tf-Idf Sparse Vector\n",
    "In Spark Maching Learning Library, Tf-Idf is separated into two parts - TF and IDF to make them flexible. Therefore, CountVectorizer is used first to generate the term frequency vector. IDF then takes feature vectors created from CountVectorizer and scales each column (token), down-weighting columns (tokens) which appear frequently in a corpus.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check vocabulary size for setting vocabSize parameters\n",
    "unfold_df = text.withColumn('word',F.explode(F.split(text.cleaned_text, \"\\s\"))).where('word != \"\"')\n",
    "vocab_df = unfold_df.groupBy(\"word\").agg(F.count(\"tokens\").alias('df')) #84084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tfidf vectors (with all words)\n",
    "# TF\n",
    "query = '''SELECT tokens, index FROM cleaned_indexed''' \n",
    "df_text = spark.sql(query)\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=84084)\n",
    "cvModel = cv.fit(df_text)\n",
    "result_cv = cvModel.transform(df_text)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) \n",
    "\n",
    "#Check DocSize and VocabSize\n",
    "tfidf = result_tfidf.select('index','features')\n",
    "print(tfidf.count(), len(cvModel.vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LDA Model on Complete Dataset and Evaluate\n",
    "By training a base LDA model with complete dataset, a couple of topics would be discovered initially and evaluated by human judgement through visulization, which could be further improved by enriching stopwords and model parameter tunning.\n",
    "#### Train LDA model with All Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "max_iterations = 100\n",
    "\n",
    "lda = LDA(featuresCol=\"features\",k=num_topics, seed=1, optimizer=\"online\", maxIterations = max_iterations)\n",
    "ldaModel=lda.fit(tfidf)\n",
    "lda_df=ldaModel.transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Top Terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|topic|words                                                                                                  |weights                                                                         |\n",
      "+-----+-------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|1    |[thyroid, cancer, thyca, support, group, tsh, conference, research, patients, page]                    |[0.019, 0.0133, 0.0103, 0.0083, 0.0071, 0.0058, 0.0053, 0.0052, 0.0044, 0.0043] |\n",
      "|2    |[bladder, bcg, cancer, urologist, cystoscopy, grade, mum, lymphedema, treatments, urine]               |[0.0229, 0.0209, 0.0073, 0.0071, 0.0061, 0.0061, 0.0053, 0.0052, 0.005, 0.0047] |\n",
      "|3    |[diet, food, water, foods, sugar, drink, oil, tea, stomach, day]                                       |[0.0088, 0.0079, 0.0066, 0.0059, 0.0047, 0.0046, 0.0042, 0.0036, 0.0035, 0.0035]|\n",
      "|4    |[aspirin, stents, scad, mountain, taxotere, statin, lad, roller, daycare, seattle]                     |[0.0051, 0.0046, 0.0039, 0.0037, 0.0036, 0.0032, 0.0031, 0.0029, 0.0028, 0.0027]|\n",
      "|5    |[day, laurie, book, voice, eyes, today, love, song, surgery, time]                                     |[0.0036, 0.0035, 0.0034, 0.003, 0.003, 0.0028, 0.0028, 0.0026, 0.0026, 0.0025]  |\n",
      "|6    |[ivf, cycle, transfer, embryos, eggs, iui, donor, egg, embryo, fet]                                    |[0.0227, 0.0175, 0.0129, 0.0114, 0.0113, 0.0089, 0.0086, 0.0086, 0.0083, 0.0076]|\n",
      "|7    |[l, gleason, psa, prostate, score, hep, cbd, cores, biopsy, harvoni]                                   |[0.011, 0.0094, 0.0085, 0.0067, 0.0066, 0.0063, 0.0058, 0.0048, 0.0045, 0.0042] |\n",
      "|8    |[encephalitis, li, propria, enfit, alzheimer, acronym, february, muscularis, connector, teeth]         |[0.012, 0.008, 0.0037, 0.0035, 0.003, 0.0029, 0.0025, 0.0024, 0.0023, 0.0021]   |\n",
      "|9    |[education, workshop, parents, disabilities, child, school, participants, children, training, students]|[0.0113, 0.0111, 0.0099, 0.0096, 0.0081, 0.0076, 0.0072, 0.0072, 0.0066, 0.0063]|\n",
      "|10   |[life, time, day, people, family, things, friends, love, way, house]                                   |[0.0066, 0.0056, 0.0052, 0.0051, 0.0051, 0.0046, 0.0043, 0.0041, 0.0038, 0.0036]|\n",
      "|11   |[pain, symptoms, doctor, years, blood, time, months, surgery, day, days]                               |[0.0132, 0.0054, 0.005, 0.0047, 0.0044, 0.0041, 0.0039, 0.0039, 0.0038, 0.0038] |\n",
      "|12   |[psa, prostate, faslodex, lupron, ibrance, uvb, prostatectomy, pc, firmagon, plexiform]                |[0.034, 0.0103, 0.0073, 0.0058, 0.0053, 0.0036, 0.0032, 0.0031, 0.0028, 0.0025] |\n",
      "|13   |[debbie, hugs, tom, cooler, membership, campers, marilyn, gang, pm, today]                             |[0.0094, 0.0084, 0.0062, 0.0058, 0.0057, 0.0056, 0.0053, 0.0049, 0.0047, 0.0047]|\n",
      "|14   |[vitamin, patients, calcium, osteoporosis, study, cells, sarcoidosis, bone, treatment, levels]         |[0.0062, 0.0053, 0.0049, 0.0047, 0.0042, 0.0041, 0.004, 0.0037, 0.0032, 0.0031] |\n",
      "|15   |[tgab, usc, quest, preservation, lion, nite, massages, spec, amn, varices]                             |[0.0085, 0.0032, 0.003, 0.0024, 0.0021, 0.0019, 0.0019, 0.0019, 0.0018, 0.0018] |\n",
      "|16   |[heart, oxygen, hospital, time, breath, day, pulmonary, breathing, exercise, days]                     |[0.0073, 0.0056, 0.0048, 0.0042, 0.004, 0.0038, 0.0034, 0.0032, 0.0032, 0.0031] |\n",
      "|17   |[stoma, bag, surgery, colostomy, pouch, ostomy, ileostomy, cancer, colon, surgeon]                     |[0.0105, 0.0093, 0.0061, 0.0058, 0.0056, 0.0049, 0.0047, 0.0046, 0.0039, 0.0035]|\n",
      "|18   |[pap, hpv, leep, cervix, leak, cin, colposcopy, hysterectomy, enbrel, endometriosis]                   |[0.0103, 0.009, 0.0048, 0.0046, 0.0041, 0.0041, 0.0041, 0.0039, 0.0037, 0.0034] |\n",
      "|19   |[tagrisso, tarceva, egfr, brain, mutation, gamma, knife, nsclc, alk, stage]                            |[0.0139, 0.01, 0.0052, 0.0048, 0.0041, 0.0041, 0.0039, 0.0034, 0.003, 0.003]    |\n",
      "|20   |[cancer, chemo, tumor, treatment, radiation, lung, liver, oncologist, scan, stage]                     |[0.0112, 0.0099, 0.0085, 0.0082, 0.0082, 0.0076, 0.007, 0.0063, 0.006, 0.006]   |\n",
      "+-----+-------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "\n",
      "Topics: 20 Vocabulary: 84084\n"
     ]
    }
   ],
   "source": [
    "#Describe top 20 topics(10 top words per topic)\n",
    "ldatopics = ldaModel.describeTopics()\n",
    "numTopics = ldatopics.count()\n",
    "\n",
    "ListOfIndexToWords = F.udf(lambda wl: list([vocabulary[w] for w in wl]), ArrayType(StringType()))\n",
    "FormatNumbers = F.udf(lambda nl: [float(\"{:1.4f}\".format(x)) for x in nl], ArrayType(FloatType()))\n",
    "\n",
    "toptopics = ldatopics.select((ldatopics.topic + 1).alias('topic'),\n",
    "                          ListOfIndexToWords(ldatopics.termIndices).alias('words'),\n",
    "                          FormatNumbers(ldatopics.termWeights).alias('weights'))\n",
    "\n",
    "toptopics.show(truncate=False, n=numTopics)\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Top Documents for Top Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tokens: array<string>, index: bigint, raw_features: vector]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"features\" does not exist.\\nAvailable fields: index, raw_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o190.transform.\n: java.lang.IllegalArgumentException: Field \"features\" does not exist.\nAvailable fields: index, raw_features\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.util.DatasetUtils$.columnToVector(DatasetUtils.scala:44)\n\tat org.apache.spark.ml.clustering.LDAModel.transform(LDA.scala:466)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-10a600a3f491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcountVectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw_features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldatopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mListOfIndexToWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldatopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermIndices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"features\" does not exist.\\nAvailable fields: index, raw_features'"
     ]
    }
   ],
   "source": [
    "countVectors = (result_cv.select(\"index\", \"raw_features\").cache())\n",
    "df = ldaModel.transform(countVectors)\n",
    "\n",
    "topWords = ldatopics.select(ListOfIndexToWords(ldatopics.termIndices).alias('words')).take(numTopics)\n",
    "\n",
    "#Show single top topic\n",
    "nTopDoc = 1  \n",
    "\n",
    "for i in range(0, numTopics):\n",
    "    ntopic = i  # which topic \n",
    "    print('Topic ' + str(ntopic) + '\\n')  \n",
    "\n",
    "    df_sliced = df.select(\"index\", \"topicDistribution\") \\\n",
    "        .rdd.map(lambda r: Row(ID=int(r[0]), weight=float(r[1][ntopic]))).toDF()\n",
    "\n",
    "    DocIDs = df_sliced.sort(df_sliced.weight.desc()).take(nTopDoc)\n",
    "    print('Top Document(s):',DocIDs)\n",
    "    for d_id in DocIDs:\n",
    "        df_text.filter(df_text.index == d_id[0]) \\\n",
    "            .select('title', 'body') \\\n",
    "            .show(truncate=False)\n",
    "\n",
    "    print('Top terms:')\n",
    "    print(topWords[ntopic][0], '\\n')\n",
    "    print('===================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count documents for each topic\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "countVectors = (result_cv.select(\"index\", \"features\").cache())\n",
    "\n",
    "countTopDocs = (ldaModel\n",
    "                .transform(countVectors)\n",
    "                .select(\"topicDistribution\")\n",
    "                .rdd.map(lambda r: Row( nTopTopic = int(np.argmax(r)))).toDF()\n",
    "                .groupBy(\"nTopTopic\").count().sort(\"nTopTopic\"))\n",
    "\n",
    "pdf = countTopDocs.toPandas()\n",
    "# pdfLess = pdf.drop(pdf.index[[1,3,7,8,10,11,14,15]]).reset_index()\n",
    "\n",
    "# pdfLess.plot(color = '#44D3A5', legend = False,\n",
    "#                            kind = 'bar', use_index = True, y = 'count', grid = False)\n",
    "# plt.xlabel('topic')\n",
    "# plt.ylabel('counts')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-190290427.02601972"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Log Likelihood of base LDA model \n",
    "ll = ldaModel.logLikelihood(tfidf)\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.434250933211537"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Log Perplexity of base LDA model\n",
    "lp = ldaModel.logPerplexity(tfidf)\n",
    "lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Base LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the model is distributed and save the model\n",
    "print(ldaModel.isDistributed())\n",
    "path = getcwd()\n",
    "model_number = '0'\n",
    "cvModel.save(path + 'CVModel_stem'+ model_number)\n",
    "ldaModel.save(path + 'LDAModel_stem'+ model_number)\n",
    "lda.save(path + 'LDA_'+ model_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvement (more meaningful topics)\n",
    "\n",
    "#### Generate TF-IDF-Term List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = cvModel.vocabulary\n",
    "ListOfIndexToWords = F.udf(lambda wl: list([vocabulary[w] for w in wl]), ArrayType(StringType()))\n",
    "ExtractValues = F.udf(lambda vec: vec.values.tolist(), ArrayType(DoubleType()))\n",
    "ExtractIndex = F.udf(lambda vec: vec.indices.tolist(), ArrayType(IntegerType()))\n",
    "result_tfidf = result_tfidf.withColumn('feature_list',ExtractValues(result_tfidf.features))\\\n",
    "                           .withColumn('term_index', ExtractIndex(result_tfidf.features))\n",
    "result_tfidf = result_tfidf.withColumn('term_list', ListOfIndexToWords(result_tfidf.term_index))\n",
    "\n",
    "tfidf_termlist = result_tfidf.select('term_list','index','feature_list')\\\n",
    "                           .withColumn(\"tmp\", F.arrays_zip('term_list','feature_list'))\\\n",
    "                           .withColumn('tmp', F.explode('tmp'))\\\n",
    "                           .select('index',F.col('tmp.term_list'),F.col('tmp.feature_list'))\\\n",
    "                           .orderBy('index','feature_list')\n",
    "\n",
    "#Preview the first entry of TF-IDF-Term list\n",
    "tfidf_termlist.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['months', 'years', 'day', 'cancer', 'year', 'thanks', 'help',\n",
       "       'time', 'pain'], dtype=object)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get terms with tfidf <= 2\n",
    "tfidf_filter_2 = tfidf_termlist.filter(tfidf_termlist.feature_list <= 2) #146126 tfidf <= 2#\n",
    "tfidf_filter_2.toPandas()['term_list'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['results', 'months', 'symptoms', 'years', 'day', 'cancer', 'year',\n",
       "       'things', 'blood', 'disease', 'body', 'family', 'diagnosis',\n",
       "       'problems', 'thanks', 'treatment', 'husband', 'stage', 'lung',\n",
       "       'doctor', 'days', 'chemo', 'help', 'way', 'need', 'hope', 'post',\n",
       "       'doctors', 'care', 'love', 'share', 'test', 'hospital',\n",
       "       'information', 'time', 'health', 'weeks', 'advice', 'thing',\n",
       "       'heart', 'week', 'work', 'area', 'dr', 'question', 'surgery',\n",
       "       'people', 'pain', 'lot', 'experience', 'times', 'morning', 'life',\n",
       "       'night', 'support', 'today', 'past', 'month', 'home', 'end',\n",
       "       'problem', 'effects', 'use', 'issues'], dtype=object)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get terms with tfidf <= 3\n",
    "tfidf_filter_3 = tfidf_termlist.filter(tfidf_termlist.feature_list <= 3)\n",
    "tfidf_filter_3.toPandas()['term_list'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time-scale words\n",
    "noun_stpwd = \\\n",
    "            ['day','month','year','days','months','years','today','everyday',\n",
    "             'time', 'weeks','dr']\n",
    "\n",
    "#Words more than 1 letters (not common for medical abbreviations)\n",
    "one_charachters = [word for word in cv_tmp_model.vocabulary if len(word) = 1]\n",
    "\n",
    "swRemover = StopWordsRemover(inputCol='tokens', outputCol=\"filtered\")\n",
    "swRemover.setStopWords(swRemover.getStopWords() + noun_stpwd +one_characers)\n",
    "\n",
    "df_text_new = swRemover.transform(df_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check document length\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_R_model(filename):\n",
    "    with open(filename, 'r') as j:\n",
    "        data_input = json.load(j)\n",
    "    data = {'topic_term_dists': data_input['phi'], \n",
    "            'doc_topic_dists': data_input['theta'],\n",
    "            'doc_lengths': data_input['doc.length'],\n",
    "            'vocab': data_input['vocab'],\n",
    "            'term_frequency': data_input['term.frequency']}\n",
    "    return data\n",
    "\n",
    "movies_model_data = load_R_model('movie_reviews_input.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[312,\n",
       " 288,\n",
       " 170,\n",
       " 435,\n",
       " 291,\n",
       " 377,\n",
       " 328,\n",
       " 276,\n",
       " 101,\n",
       " 226,\n",
       " 391,\n",
       " 271,\n",
       " 140,\n",
       " 156,\n",
       " 437,\n",
       " 248,\n",
       " 167,\n",
       " 362,\n",
       " 210,\n",
       " 252,\n",
       " 155,\n",
       " 267,\n",
       " 188,\n",
       " 333,\n",
       " 366,\n",
       " 357,\n",
       " 190,\n",
       " 565,\n",
       " 394,\n",
       " 427,\n",
       " 367,\n",
       " 538,\n",
       " 321,\n",
       " 474,\n",
       " 267,\n",
       " 210,\n",
       " 155,\n",
       " 344,\n",
       " 123,\n",
       " 230,\n",
       " 269,\n",
       " 474,\n",
       " 291,\n",
       " 228,\n",
       " 323,\n",
       " 349,\n",
       " 331,\n",
       " 244,\n",
       " 228,\n",
       " 133,\n",
       " 158,\n",
       " 278,\n",
       " 132,\n",
       " 332,\n",
       " 196,\n",
       " 277,\n",
       " 223,\n",
       " 60,\n",
       " 108,\n",
       " 399,\n",
       " 348,\n",
       " 298,\n",
       " 448,\n",
       " 131,\n",
       " 284,\n",
       " 303,\n",
       " 247,\n",
       " 212,\n",
       " 264,\n",
       " 282,\n",
       " 245,\n",
       " 360,\n",
       " 196,\n",
       " 334,\n",
       " 573,\n",
       " 96,\n",
       " 490,\n",
       " 219,\n",
       " 249,\n",
       " 144,\n",
       " 183,\n",
       " 251,\n",
       " 119,\n",
       " 323,\n",
       " 311,\n",
       " 274,\n",
       " 435,\n",
       " 330,\n",
       " 274,\n",
       " 169,\n",
       " 147,\n",
       " 175,\n",
       " 291,\n",
       " 133,\n",
       " 264,\n",
       " 245,\n",
       " 332,\n",
       " 295,\n",
       " 166,\n",
       " 380,\n",
       " 239,\n",
       " 332,\n",
       " 259,\n",
       " 271,\n",
       " 214,\n",
       " 474,\n",
       " 231,\n",
       " 213,\n",
       " 141,\n",
       " 281,\n",
       " 374,\n",
       " 123,\n",
       " 352,\n",
       " 323,\n",
       " 82,\n",
       " 542,\n",
       " 337,\n",
       " 301,\n",
       " 287,\n",
       " 128,\n",
       " 402,\n",
       " 321,\n",
       " 581,\n",
       " 498,\n",
       " 263,\n",
       " 190,\n",
       " 324,\n",
       " 240,\n",
       " 160,\n",
       " 194,\n",
       " 154,\n",
       " 342,\n",
       " 105,\n",
       " 228,\n",
       " 262,\n",
       " 214,\n",
       " 369,\n",
       " 165,\n",
       " 225,\n",
       " 190,\n",
       " 366,\n",
       " 288,\n",
       " 279,\n",
       " 453,\n",
       " 167,\n",
       " 109,\n",
       " 673,\n",
       " 336,\n",
       " 236,\n",
       " 209,\n",
       " 217,\n",
       " 244,\n",
       " 163,\n",
       " 305,\n",
       " 300,\n",
       " 273,\n",
       " 182,\n",
       " 122,\n",
       " 393,\n",
       " 317,\n",
       " 258,\n",
       " 144,\n",
       " 701,\n",
       " 602,\n",
       " 251,\n",
       " 373,\n",
       " 349,\n",
       " 276,\n",
       " 205,\n",
       " 395,\n",
       " 118,\n",
       " 284,\n",
       " 231,\n",
       " 174,\n",
       " 230,\n",
       " 185,\n",
       " 171,\n",
       " 335,\n",
       " 307,\n",
       " 191,\n",
       " 218,\n",
       " 233,\n",
       " 242,\n",
       " 263,\n",
       " 109,\n",
       " 257,\n",
       " 287,\n",
       " 231,\n",
       " 186,\n",
       " 237,\n",
       " 266,\n",
       " 216,\n",
       " 239,\n",
       " 231,\n",
       " 164,\n",
       " 220,\n",
       " 307,\n",
       " 124,\n",
       " 137,\n",
       " 342,\n",
       " 194,\n",
       " 272,\n",
       " 315,\n",
       " 361,\n",
       " 189,\n",
       " 287,\n",
       " 473,\n",
       " 295,\n",
       " 305,\n",
       " 148,\n",
       " 177,\n",
       " 551,\n",
       " 179,\n",
       " 86,\n",
       " 177,\n",
       " 265,\n",
       " 196,\n",
       " 137,\n",
       " 182,\n",
       " 179,\n",
       " 411,\n",
       " 146,\n",
       " 257,\n",
       " 273,\n",
       " 306,\n",
       " 241,\n",
       " 223,\n",
       " 217,\n",
       " 254,\n",
       " 88,\n",
       " 89,\n",
       " 481,\n",
       " 236,\n",
       " 378,\n",
       " 289,\n",
       " 157,\n",
       " 350,\n",
       " 167,\n",
       " 234,\n",
       " 298,\n",
       " 399,\n",
       " 410,\n",
       " 155,\n",
       " 256,\n",
       " 183,\n",
       " 200,\n",
       " 364,\n",
       " 347,\n",
       " 407,\n",
       " 171,\n",
       " 387,\n",
       " 285,\n",
       " 466,\n",
       " 136,\n",
       " 282,\n",
       " 198,\n",
       " 715,\n",
       " 228,\n",
       " 309,\n",
       " 239,\n",
       " 159,\n",
       " 366,\n",
       " 399,\n",
       " 224,\n",
       " 187,\n",
       " 245,\n",
       " 285,\n",
       " 363,\n",
       " 190,\n",
       " 549,\n",
       " 92,\n",
       " 326,\n",
       " 160,\n",
       " 308,\n",
       " 542,\n",
       " 370,\n",
       " 260,\n",
       " 106,\n",
       " 501,\n",
       " 217,\n",
       " 53,\n",
       " 312,\n",
       " 352,\n",
       " 243,\n",
       " 462,\n",
       " 186,\n",
       " 388,\n",
       " 146,\n",
       " 264,\n",
       " 305,\n",
       " 323,\n",
       " 222,\n",
       " 189,\n",
       " 162,\n",
       " 357,\n",
       " 210,\n",
       " 240,\n",
       " 254,\n",
       " 434,\n",
       " 201,\n",
       " 352,\n",
       " 163,\n",
       " 288,\n",
       " 405,\n",
       " 358,\n",
       " 228,\n",
       " 367,\n",
       " 287,\n",
       " 175,\n",
       " 668,\n",
       " 334,\n",
       " 373,\n",
       " 144,\n",
       " 339,\n",
       " 169,\n",
       " 440,\n",
       " 207,\n",
       " 590,\n",
       " 329,\n",
       " 120,\n",
       " 522,\n",
       " 294,\n",
       " 165,\n",
       " 142,\n",
       " 264,\n",
       " 224,\n",
       " 126,\n",
       " 569,\n",
       " 556,\n",
       " 134,\n",
       " 209,\n",
       " 310,\n",
       " 404,\n",
       " 409,\n",
       " 289,\n",
       " 376,\n",
       " 399,\n",
       " 313,\n",
       " 140,\n",
       " 174,\n",
       " 249,\n",
       " 369,\n",
       " 225,\n",
       " 199,\n",
       " 190,\n",
       " 673,\n",
       " 243,\n",
       " 208,\n",
       " 108,\n",
       " 77,\n",
       " 154,\n",
       " 188,\n",
       " 174,\n",
       " 150,\n",
       " 259,\n",
       " 218,\n",
       " 313,\n",
       " 337,\n",
       " 317,\n",
       " 186,\n",
       " 265,\n",
       " 138,\n",
       " 222,\n",
       " 190,\n",
       " 213,\n",
       " 314,\n",
       " 244,\n",
       " 174,\n",
       " 353,\n",
       " 352,\n",
       " 242,\n",
       " 434,\n",
       " 80,\n",
       " 225,\n",
       " 256,\n",
       " 182,\n",
       " 213,\n",
       " 221,\n",
       " 186,\n",
       " 307,\n",
       " 155,\n",
       " 337,\n",
       " 331,\n",
       " 396,\n",
       " 154,\n",
       " 214,\n",
       " 361,\n",
       " 106,\n",
       " 306,\n",
       " 384,\n",
       " 124,\n",
       " 466,\n",
       " 108,\n",
       " 265,\n",
       " 100,\n",
       " 265,\n",
       " 249,\n",
       " 414,\n",
       " 402,\n",
       " 298,\n",
       " 115,\n",
       " 502,\n",
       " 287,\n",
       " 83,\n",
       " 200,\n",
       " 248,\n",
       " 240,\n",
       " 247,\n",
       " 218,\n",
       " 323,\n",
       " 429,\n",
       " 669,\n",
       " 225,\n",
       " 310,\n",
       " 454,\n",
       " 292,\n",
       " 142,\n",
       " 224,\n",
       " 135,\n",
       " 179,\n",
       " 252,\n",
       " 653,\n",
       " 335,\n",
       " 261,\n",
       " 117,\n",
       " 88,\n",
       " 274,\n",
       " 371,\n",
       " 164,\n",
       " 161,\n",
       " 139,\n",
       " 204,\n",
       " 448,\n",
       " 516,\n",
       " 312,\n",
       " 532,\n",
       " 512,\n",
       " 371,\n",
       " 214,\n",
       " 560,\n",
       " 314,\n",
       " 386,\n",
       " 506,\n",
       " 155,\n",
       " 185,\n",
       " 381,\n",
       " 183,\n",
       " 241,\n",
       " 172,\n",
       " 203,\n",
       " 169,\n",
       " 287,\n",
       " 215,\n",
       " 451,\n",
       " 296,\n",
       " 319,\n",
       " 286,\n",
       " 297,\n",
       " 216,\n",
       " 140,\n",
       " 246,\n",
       " 385,\n",
       " 376,\n",
       " 296,\n",
       " 232,\n",
       " 174,\n",
       " 246,\n",
       " 366,\n",
       " 273,\n",
       " 516,\n",
       " 364,\n",
       " 104,\n",
       " 278,\n",
       " 123,\n",
       " 226,\n",
       " 384,\n",
       " 303,\n",
       " 286,\n",
       " 257,\n",
       " 274,\n",
       " 174,\n",
       " 213,\n",
       " 296,\n",
       " 194,\n",
       " 164,\n",
       " 224,\n",
       " 293,\n",
       " 310,\n",
       " 313,\n",
       " 209,\n",
       " 441,\n",
       " 152,\n",
       " 391,\n",
       " 157,\n",
       " 245,\n",
       " 258,\n",
       " 371,\n",
       " 265,\n",
       " 186,\n",
       " 214,\n",
       " 321,\n",
       " 639,\n",
       " 345,\n",
       " 378,\n",
       " 243,\n",
       " 609,\n",
       " 556,\n",
       " 222,\n",
       " 492,\n",
       " 407,\n",
       " 278,\n",
       " 412,\n",
       " 715,\n",
       " 105,\n",
       " 191,\n",
       " 160,\n",
       " 348,\n",
       " 106,\n",
       " 309,\n",
       " 532,\n",
       " 357,\n",
       " 348,\n",
       " 242,\n",
       " 303,\n",
       " 243,\n",
       " 347,\n",
       " 313,\n",
       " 179,\n",
       " 329,\n",
       " 264,\n",
       " 219,\n",
       " 333,\n",
       " 484,\n",
       " 187,\n",
       " 148,\n",
       " 100,\n",
       " 207,\n",
       " 324,\n",
       " 361,\n",
       " 145,\n",
       " 278,\n",
       " 326,\n",
       " 94,\n",
       " 296,\n",
       " 242,\n",
       " 491,\n",
       " 351,\n",
       " 194,\n",
       " 373,\n",
       " 327,\n",
       " 280,\n",
       " 342,\n",
       " 1124,\n",
       " 380,\n",
       " 312,\n",
       " 196,\n",
       " 488,\n",
       " 468,\n",
       " 352,\n",
       " 300,\n",
       " 594,\n",
       " 273,\n",
       " 202,\n",
       " 533,\n",
       " 166,\n",
       " 156,\n",
       " 343,\n",
       " 302,\n",
       " 119,\n",
       " 323,\n",
       " 271,\n",
       " 407,\n",
       " 342,\n",
       " 341,\n",
       " 277,\n",
       " 234,\n",
       " 212,\n",
       " 442,\n",
       " 382,\n",
       " 250,\n",
       " 214,\n",
       " 370,\n",
       " 434,\n",
       " 309,\n",
       " 173,\n",
       " 377,\n",
       " 240,\n",
       " 178,\n",
       " 191,\n",
       " 438,\n",
       " 294,\n",
       " 334,\n",
       " 166,\n",
       " 228,\n",
       " 255,\n",
       " 270,\n",
       " 272,\n",
       " 943,\n",
       " 95,\n",
       " 285,\n",
       " 517,\n",
       " 737,\n",
       " 219,\n",
       " 302,\n",
       " 337,\n",
       " 246,\n",
       " 542,\n",
       " 162,\n",
       " 328,\n",
       " 343,\n",
       " 202,\n",
       " 185,\n",
       " 579,\n",
       " 180,\n",
       " 280,\n",
       " 229,\n",
       " 269,\n",
       " 218,\n",
       " 404,\n",
       " 231,\n",
       " 453,\n",
       " 460,\n",
       " 258,\n",
       " 290,\n",
       " 357,\n",
       " 144,\n",
       " 280,\n",
       " 299,\n",
       " 285,\n",
       " 186,\n",
       " 241,\n",
       " 340,\n",
       " 385,\n",
       " 519,\n",
       " 420,\n",
       " 366,\n",
       " 323,\n",
       " 205,\n",
       " 659,\n",
       " 341,\n",
       " 84,\n",
       " 354,\n",
       " 336,\n",
       " 187,\n",
       " 590,\n",
       " 411,\n",
       " 237,\n",
       " 320,\n",
       " 260,\n",
       " 340,\n",
       " 286,\n",
       " 276,\n",
       " 156,\n",
       " 353,\n",
       " 304,\n",
       " 284,\n",
       " 204,\n",
       " 319,\n",
       " 356,\n",
       " 209,\n",
       " 560,\n",
       " 313,\n",
       " 250,\n",
       " 417,\n",
       " 235,\n",
       " 266,\n",
       " 459,\n",
       " 545,\n",
       " 488,\n",
       " 214,\n",
       " 246,\n",
       " 208,\n",
       " 308,\n",
       " 624,\n",
       " 402,\n",
       " 391,\n",
       " 300,\n",
       " 312,\n",
       " 493,\n",
       " 268,\n",
       " 229,\n",
       " 171,\n",
       " 291,\n",
       " 588,\n",
       " 227,\n",
       " 303,\n",
       " 84,\n",
       " 274,\n",
       " 185,\n",
       " 276,\n",
       " 109,\n",
       " 235,\n",
       " 626,\n",
       " 366,\n",
       " 244,\n",
       " 275,\n",
       " 292,\n",
       " 210,\n",
       " 547,\n",
       " 250,\n",
       " 512,\n",
       " 437,\n",
       " 230,\n",
       " 139,\n",
       " 409,\n",
       " 165,\n",
       " 504,\n",
       " 397,\n",
       " 329,\n",
       " 221,\n",
       " 275,\n",
       " 422,\n",
       " 267,\n",
       " 194,\n",
       " 258,\n",
       " 138,\n",
       " 291,\n",
       " 550,\n",
       " 117,\n",
       " 406,\n",
       " 284,\n",
       " 210,\n",
       " 117,\n",
       " 164,\n",
       " 235,\n",
       " 211,\n",
       " 180,\n",
       " 168,\n",
       " 460,\n",
       " 473,\n",
       " 365,\n",
       " 292,\n",
       " 190,\n",
       " 396,\n",
       " 311,\n",
       " 376,\n",
       " 452,\n",
       " 323,\n",
       " 547,\n",
       " 342,\n",
       " 262,\n",
       " 247,\n",
       " 156,\n",
       " 342,\n",
       " 277,\n",
       " 309,\n",
       " 436,\n",
       " 159,\n",
       " 155,\n",
       " 226,\n",
       " 382,\n",
       " 171,\n",
       " 310,\n",
       " 284,\n",
       " 56,\n",
       " 222,\n",
       " 202,\n",
       " 338,\n",
       " 343,\n",
       " 232,\n",
       " 124,\n",
       " 379,\n",
       " 221,\n",
       " 121,\n",
       " 148,\n",
       " 904,\n",
       " 210,\n",
       " 115,\n",
       " 300,\n",
       " 317,\n",
       " 346,\n",
       " 546,\n",
       " 418,\n",
       " 244,\n",
       " 122,\n",
       " 158,\n",
       " 256,\n",
       " 341,\n",
       " 244,\n",
       " 274,\n",
       " 453,\n",
       " 316,\n",
       " 357,\n",
       " 450,\n",
       " 227,\n",
       " 407,\n",
       " 253,\n",
       " 223,\n",
       " 164,\n",
       " 274,\n",
       " 327,\n",
       " 291,\n",
       " 476,\n",
       " 284,\n",
       " 349,\n",
       " 251,\n",
       " 273,\n",
       " 70,\n",
       " 364,\n",
       " 202,\n",
       " 289,\n",
       " 401,\n",
       " 423,\n",
       " 357,\n",
       " 318,\n",
       " 313,\n",
       " 295,\n",
       " 425,\n",
       " 323,\n",
       " 404,\n",
       " 437,\n",
       " 320,\n",
       " 431,\n",
       " 206,\n",
       " 101,\n",
       " 273,\n",
       " 259,\n",
       " 564,\n",
       " 406,\n",
       " 495,\n",
       " 197,\n",
       " 244,\n",
       " 325,\n",
       " 180,\n",
       " 171,\n",
       " 70,\n",
       " 245,\n",
       " 228,\n",
       " 274,\n",
       " 222,\n",
       " 320,\n",
       " 223,\n",
       " 264,\n",
       " 291,\n",
       " 278,\n",
       " 76,\n",
       " 496,\n",
       " 393,\n",
       " 214,\n",
       " 342,\n",
       " 232,\n",
       " 96,\n",
       " 249,\n",
       " 135,\n",
       " 385,\n",
       " 416,\n",
       " 380,\n",
       " 92,\n",
       " 225,\n",
       " 265,\n",
       " 173,\n",
       " 250,\n",
       " 233,\n",
       " 260,\n",
       " 262,\n",
       " 235,\n",
       " 341,\n",
       " 198,\n",
       " 425,\n",
       " 332,\n",
       " 337,\n",
       " 156,\n",
       " 516,\n",
       " 178,\n",
       " 187,\n",
       " 312,\n",
       " 345,\n",
       " 147,\n",
       " 341,\n",
       " 361,\n",
       " 409,\n",
       " 288,\n",
       " 493,\n",
       " 378,\n",
       " 328,\n",
       " 354,\n",
       " 122,\n",
       " 153,\n",
       " 305,\n",
       " 323,\n",
       " 228,\n",
       " 119,\n",
       " 236,\n",
       " 182,\n",
       " 330,\n",
       " 195,\n",
       " 155,\n",
       " 381,\n",
       " 341,\n",
       " 303,\n",
       " 196,\n",
       " 227,\n",
       " 285,\n",
       " 570,\n",
       " 192,\n",
       " 395,\n",
       " 370,\n",
       " 322,\n",
       " 264,\n",
       " 205,\n",
       " 310,\n",
       " 115,\n",
       " 471,\n",
       " 304,\n",
       " 162,\n",
       " 364,\n",
       " 160,\n",
       " 294,\n",
       " 596,\n",
       " 242,\n",
       " 441,\n",
       " 239,\n",
       " 222,\n",
       " 318,\n",
       " 155,\n",
       " 182,\n",
       " 258,\n",
       " 241,\n",
       " 221,\n",
       " 259,\n",
       " 241,\n",
       " 206,\n",
       " 660,\n",
       " 198,\n",
       " 250,\n",
       " 288,\n",
       " 212,\n",
       " 338,\n",
       " 448,\n",
       " 376,\n",
       " 410,\n",
       " 136,\n",
       " 224,\n",
       " 363,\n",
       " 200,\n",
       " 595,\n",
       " 554,\n",
       " 213,\n",
       " 101,\n",
       " 220,\n",
       " 285,\n",
       " 266,\n",
       " 225,\n",
       " 158,\n",
       " 357,\n",
       " 385,\n",
       " 856,\n",
       " 250,\n",
       " 171,\n",
       " 369,\n",
       " 219,\n",
       " 222,\n",
       " 268,\n",
       " 330,\n",
       " 209,\n",
       " 401,\n",
       " 336,\n",
       " 188,\n",
       " 323,\n",
       " 181,\n",
       " 384,\n",
       " 206,\n",
       " 206,\n",
       " 296,\n",
       " 403,\n",
       " 216,\n",
       " 281,\n",
       " 497,\n",
       " 281,\n",
       " 252,\n",
       " 347,\n",
       " 260,\n",
       " 224,\n",
       " 440,\n",
       " 183,\n",
       " 381,\n",
       " 78,\n",
       " 175,\n",
       " 334,\n",
       " 317,\n",
       " 182,\n",
       " 339,\n",
       " 232,\n",
       " 319,\n",
       " 241,\n",
       " 271,\n",
       " 395,\n",
       " 79,\n",
       " 388,\n",
       " 312,\n",
       " 414,\n",
       " 228,\n",
       " 528,\n",
       " 248,\n",
       " 227,\n",
       " 286,\n",
       " 132,\n",
       " 482,\n",
       " 262,\n",
       " 409,\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "movies_model_data['doc_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
