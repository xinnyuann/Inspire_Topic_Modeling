{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import ftfy\n",
    "import ftfy.bad_codecs\n",
    "import emoji\n",
    "import langid\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "import spacy\n",
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA, TAG\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_json(\"cleaning_test_output.json\", orient = 'records', lines = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 1 µs, total: 9 µs\n",
      "Wall time: 19.1 µs\n",
      "['emoji_remover', 'tagger', 'parser', 'ner', 'remove_tokens_onmatch']\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import spacy\n",
    "\n",
    "def emoji_remover(doc):\n",
    "    def keep_token(token):\n",
    "        if not any(j in token.text for j in emoji.UNICODE_EMOJI):\n",
    "            return token\n",
    "    emoji_removed = list(filter(keep_token,doc))\n",
    "    result = ' '.join(token.string for token in emoji_removed)\n",
    "    return tokenizer(result)\n",
    "\n",
    "def remove_tokens_onmatch(doc):\n",
    "    def keep_token(token):\n",
    "        if (token.pos_ not in ('PUNCT','SYM','SPACE')) & (token.tag_ not in {\"URL\",\"EMAIL\"}):\n",
    "            return token\n",
    "    removed_result = list(filter(keep_token,doc))\n",
    "    result = ''.join(token.string for token in removed_result)\n",
    "    return tokenizer(result)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(emoji_remover, name='emoji_remover', first=True)\n",
    "nlp.add_pipe(remove_tokens_onmatch, name='remove_tokens_onmatch', first=False)\n",
    "tokenizer = English().Defaults.create_tokenizer(nlp)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_cleaned.body.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 37.9 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df_words = list(map(lambda x : nlp(u\"{}\".format(x)), test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['semi_cleaned'] = np.array(df_words).reshape((len(df_words),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "  \n",
    "# initializing string   \n",
    "test_string = \"\"\n",
    "  \n",
    "# using sum() + strip() + split() \n",
    "# to count words in string \n",
    "res = sum([i.strip(string.punctuation).isalpha() for i in test_string.split()]) \n",
    "  \n",
    "# printing result \n",
    "print (\"The number of words in string are : \" +  str(res)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_length (df_str):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
